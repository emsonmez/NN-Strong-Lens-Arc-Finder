{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660bfe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astroquery.mast import Catalogs\n",
    "from astropy import units as u\n",
    "from astropy.io import fits\n",
    "from astropy.io.fits import CompImageHDU, getheader\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import requests\n",
    "import urllib\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3282af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1: Extract SDSS DR18 LRG Samples using CasJobs\n",
    "# Data listed in dr18_lrg_sample.csv (Sample Size: 1,174,900)\n",
    "# Criteria : \n",
    "\n",
    "# SELECT\n",
    "  # s.specobjid,\n",
    "  # s.z AS redshift,                   \n",
    "  # s.veldisp,\n",
    "  # p.ra, p.dec, p.u, p.g, p.r, p.i, p.modelMag_r,\n",
    "  # s.programname, s.plate, s.fiberid, s.mjd into mydb.LRG_full_catalog from SpecObjAll AS s\n",
    "# JOIN PhotoObjAll AS p ON s.bestobjid = p.objid\n",
    "# WHERE\n",
    "  # s.class = 'GALAXY'\n",
    "  # AND s.z BETWEEN 0.1 AND 0.7\n",
    "  # AND s.veldisp > 0 AND s.veldisp < 500\n",
    "  # AND s.programname IN ('boss', 'eboss')\n",
    "  # AND (p.r - p.i) > 0.5\n",
    "  # AND (p.g - p.r) > 0.7\n",
    "  # AND p.modelMag_r BETWEEN 16 AND 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-match SDSS data with the PS1 catalog\n",
    "\n",
    "BATCH_SIZE = 10000\n",
    "INPUT_CSV = \"sdss_dr18_lrg_sample.csv\"\n",
    "OUTPUT_CSV_TEMPLATE = \"sdss_lrg_ps1_matched_batch_{batch_num}.csv\"\n",
    "OUTPUT_FOLDER = \"sdss_lrg_queried_objects\"\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "n_objects = len(df)\n",
    "\n",
    "def query_ps1_batch(batch_df):\n",
    "    g_mag, r_mag, i_mag = [], [], []\n",
    "    g_depth, r_depth, i_depth = [], [], []\n",
    "    g_fwhm, r_fwhm, i_fwhm = [], [], []\n",
    "\n",
    "    for _, row in tqdm(batch_df.iterrows(), total=len(batch_df)):\n",
    "        try:\n",
    "            coord = SkyCoord(ra=row['ra'], dec=row['dec'], unit='deg', frame='icrs')\n",
    "            result = Catalogs.query_region(coord, radius=2.5 * u.arcsec, catalog='PanSTARRS', data_release='dr2')\n",
    "            if len(result) > 0:\n",
    "                best = result[0]\n",
    "                g_mag.append(best.get('gMeanPSFMag'))\n",
    "                r_mag.append(best.get('rMeanPSFMag'))\n",
    "                i_mag.append(best.get('iMeanPSFMag'))\n",
    "\n",
    "                g_depth.append(best.get('gMeanDepth'))\n",
    "                r_depth.append(best.get('rMeanDepth'))\n",
    "                i_depth.append(best.get('iMeanDepth'))\n",
    "\n",
    "                g_fwhm.append(best.get('gFWHM'))\n",
    "                r_fwhm.append(best.get('rFWHM'))\n",
    "                i_fwhm.append(best.get('iFWHM'))\n",
    "            else:\n",
    "                g_mag.append(None)\n",
    "                r_mag.append(None)\n",
    "                i_mag.append(None)\n",
    "                g_depth.append(None)\n",
    "                r_depth.append(None)\n",
    "                i_depth.append(None)\n",
    "                g_fwhm.append(None)\n",
    "                r_fwhm.append(None)\n",
    "                i_fwhm.append(None)\n",
    "        except Exception:\n",
    "            g_mag.append(None)\n",
    "            r_mag.append(None)\n",
    "            i_mag.append(None)\n",
    "            g_depth.append(None)\n",
    "            r_depth.append(None)\n",
    "            i_depth.append(None)\n",
    "            g_fwhm.append(None)\n",
    "            r_fwhm.append(None)\n",
    "            i_fwhm.append(None)\n",
    "\n",
    "    batch_df['ps1_g_mag'] = g_mag\n",
    "    batch_df['ps1_r_mag'] = r_mag\n",
    "    batch_df['ps1_i_mag'] = i_mag\n",
    "    batch_df['ps1_g_depth'] = g_depth\n",
    "    batch_df['ps1_r_depth'] = r_depth\n",
    "    batch_df['ps1_i_depth'] = i_depth\n",
    "    batch_df['ps1_g_fwhm'] = g_fwhm\n",
    "    batch_df['ps1_r_fwhm'] = r_fwhm\n",
    "    batch_df['ps1_i_fwhm'] = i_fwhm\n",
    "\n",
    "    # Filter: keep only rows with at least 2 non-null PS1 magnitudes\n",
    "    mask = (\n",
    "        batch_df[['ps1_g_mag', 'ps1_r_mag', 'ps1_i_mag']]\n",
    "        .notnull()\n",
    "        .sum(axis=1) >= 2\n",
    "    )\n",
    "    filtered_df = batch_df[mask].reset_index(drop=True)\n",
    "    print(f\"Filtered from {len(batch_df)} → {len(filtered_df)} rows with ≥ 2 PS1 bands\")\n",
    "    return filtered_df\n",
    "\n",
    "for i in range(0, n_objects, BATCH_SIZE):\n",
    "    batch_num = i // BATCH_SIZE + 1\n",
    "    output_file = os.path.join(OUTPUT_FOLDER, OUTPUT_CSV_TEMPLATE.format(batch_num=batch_num))\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Batch {batch_num} already done (found {output_file}), skipping...\")\n",
    "        continue\n",
    "\n",
    "    batch_df = df.iloc[i:i+BATCH_SIZE].copy()\n",
    "    print(f\"\\nProcessing batch {batch_num} ({i} to {i + len(batch_df) - 1})...\")\n",
    "    filtered_df = query_ps1_batch(batch_df)\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "    filtered_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved filtered batch {batch_num} to {output_file}\")\n",
    "\n",
    "print(\"\\nAll batches processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c50f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating color of each object\n",
    "# Prioritize g-r, then r-i, then g-i, then paste them in each csv file \n",
    "\n",
    "INPUT_FOLDER = \"sdss_lrg_queried_objects\"\n",
    "CHUNK_SIZE = 10000\n",
    "\n",
    "def determine_color(row):\n",
    "    g, r, i = row['ps1_g_mag'], row['ps1_r_mag'], row['ps1_i_mag']\n",
    "    if pd.notnull(g) and pd.notnull(r):\n",
    "        row['color_name'] = 'g-r'\n",
    "        row['color_value'] = g - r\n",
    "    elif pd.notnull(r) and pd.notnull(i):\n",
    "        row['color_name'] = 'r-i'\n",
    "        row['color_value'] = r - i\n",
    "    elif pd.notnull(g) and pd.notnull(i):\n",
    "        row['color_name'] = 'g-i'\n",
    "        row['color_value'] = g - i\n",
    "    else:\n",
    "        row['color_name'] = None\n",
    "        row['color_value'] = None\n",
    "    return row\n",
    "\n",
    "for filename in sorted(os.listdir(INPUT_FOLDER)):\n",
    "    if not filename.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    input_path = os.path.join(INPUT_FOLDER, filename)\n",
    "    temp_output_path = os.path.join(INPUT_FOLDER, f\"temp_{filename}\")\n",
    "    first_batch = True\n",
    "\n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "\n",
    "    for chunk in pd.read_csv(input_path, chunksize=CHUNK_SIZE):\n",
    "        # Replace '--' with NaN and convert to float\n",
    "        for col in ['ps1_g_mag', 'ps1_r_mag', 'ps1_i_mag']:\n",
    "            if col in chunk.columns:\n",
    "                chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n",
    "\n",
    "        # Apply color computation\n",
    "        chunk = chunk.apply(determine_color, axis=1)\n",
    "\n",
    "        # Save to temp file\n",
    "        chunk.to_csv(\n",
    "            temp_output_path,\n",
    "            mode='w' if first_batch else 'a',\n",
    "            index=False,\n",
    "            header=first_batch\n",
    "        )\n",
    "        first_batch = False\n",
    "        print(f\"  Processed chunk with {len(chunk)} rows\")\n",
    "\n",
    "    # Overwrite original file\n",
    "    os.replace(temp_output_path, input_path)\n",
    "    print(f\"Updated file saved: {filename}\")\n",
    "\n",
    "print(\"All CSVs processed with color info added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading PS1 cutouts for each object that has a valid PS1 magnitude \n",
    "\n",
    "nest_asyncio.apply()  # For Jupyter\n",
    "\n",
    "INPUT_FOLDER = \"sdss_lrg_queried_objects\"\n",
    "OUTPUT_FOLDER = \"cutouts\"\n",
    "FILTERS = ['g', 'r', 'i']\n",
    "CUTOUT_SIZE = 80\n",
    "MIN_FITS_BYTES = 10_000\n",
    "MAX_CONCURRENT_REQUESTS = 50  # Tune this depending on your bandwidth\n",
    "\n",
    "# Ensure output folders exist\n",
    "for band in FILTERS:\n",
    "    os.makedirs(os.path.join(OUTPUT_FOLDER, band), exist_ok=True)\n",
    "\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "def is_valid_mag(val):\n",
    "    return not (pd.isna(val) or (isinstance(val, str) and val.strip() == \"--\"))\n",
    "\n",
    "async def fetch_with_retries(session, url, timeout=15, retries=3, backoff=1.5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with session.get(url, timeout=timeout) as resp:\n",
    "                if resp.status == 200:\n",
    "                    return await resp.read()\n",
    "                else:\n",
    "                    raise Exception(f\"HTTP {resp.status}\")\n",
    "        except Exception as e:\n",
    "            if attempt < retries - 1:\n",
    "                await asyncio.sleep(backoff ** attempt)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "async def download_band_fits(session, specobjid, ra, dec, band):\n",
    "    async with semaphore:\n",
    "        save_path = os.path.join(OUTPUT_FOLDER, band, f\"{specobjid}_{band}.fits\")\n",
    "        if os.path.exists(save_path):\n",
    "            return f\"Skipped {specobjid} {band} (already exists)\"\n",
    "\n",
    "        # Step 1: Get full FITS file path from ps1filenames.py\n",
    "        script_url = f\"http://ps1images.stsci.edu/cgi-bin/ps1filenames.py?ra={ra}&dec={dec}&filters={band}\"\n",
    "\n",
    "        try:\n",
    "            text = (await fetch_with_retries(session, script_url)).decode(\"utf-8\")\n",
    "            lines = text.strip().splitlines()\n",
    "            if len(lines) < 2:\n",
    "                return f\"No FITS entry for {specobjid} {band}\"\n",
    "\n",
    "            fits_path = lines[1].split()[7]  # full FITS file path on server\n",
    "\n",
    "            # Step 2: Build FITS cutout URL with parameters\n",
    "            base_cutout_url = \"https://ps1images.stsci.edu/cgi-bin/fitscut.cgi\"\n",
    "            cutout_url = (\n",
    "                f\"{base_cutout_url}\"\n",
    "                f\"?red={fits_path}\"\n",
    "                f\"&format=fits\"\n",
    "                f\"&x={ra}\"\n",
    "                f\"&y={dec}\"\n",
    "                f\"&size={CUTOUT_SIZE}\"\n",
    "                f\"&wcs=1\"\n",
    "                f\"&imagename=cutout_{os.path.basename(fits_path)}\"\n",
    "            )\n",
    "\n",
    "            content = await fetch_with_retries(session, cutout_url, timeout=30)\n",
    "\n",
    "            if len(content) < MIN_FITS_BYTES:\n",
    "                return f\"Invalid FITS cutout too small for {specobjid} {band}\"\n",
    "\n",
    "            with fits.open(BytesIO(content)) as hdu:\n",
    "                fits.writeto(save_path, hdu[0].data, hdu[0].header, overwrite=True)\n",
    "\n",
    "            return f\"Downloaded cutout {specobjid} {band}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error downloading cutout {specobjid} {band}: {e}\"\n",
    "\n",
    "async def process_row(session, specobjid, ra, dec, row):\n",
    "    tasks = []\n",
    "    for band in FILTERS:\n",
    "        mag_col = f\"ps1_{band}_mag\"\n",
    "        if mag_col in row and is_valid_mag(row[mag_col]):\n",
    "            tasks.append(download_band_fits(session, specobjid, ra, dec, band))\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    for r in results:\n",
    "        print(r)\n",
    "\n",
    "async def process_file(session, filepath, index, total):\n",
    "    print(f\"[{index}/{total}] Processing {filepath}...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    for _, row in df.iterrows():\n",
    "        specobjid = str(int(row['specobjid']))\n",
    "        ra = row['ra']\n",
    "        dec = row['dec']\n",
    "        await process_row(session, specobjid, ra, dec, row)\n",
    "    print(f\"Finished {filepath}\")\n",
    "\n",
    "async def main():\n",
    "    csv_files = sorted(f for f in os.listdir(INPUT_FOLDER) if f.endswith(\".csv\"))\n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for idx, filename in enumerate(csv_files, 1):\n",
    "            path = os.path.join(INPUT_FOLDER, filename)\n",
    "            await process_file(session, path, idx, len(csv_files))\n",
    "\n",
    "# Run in Jupyter:\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
